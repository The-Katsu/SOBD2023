{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Выполните анализ Вами выбранного датасета с помощью двух алгоритмов машинного\n",
    "обучения в соответствии с индивидуальным вариантом:\n",
    "\n",
    "Вариант: 1  \n",
    "Задача регрессии: RandomForest  \n",
    "Задача бинарной классификации: LogisticRegression\n",
    "\n",
    "Датасет: 5. Датасет исторических данных по фотоэлектричеству и нагрузке."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "400fec1364b9ff29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:27:36.404361400Z",
     "start_time": "2024-01-18T15:27:36.311936500Z"
    }
   },
   "id": "6d3ae901be3a6eda",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d832c93266f8615"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = '20G'\n",
    "# Initialize a spark session.\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n",
    "        .set('spark.executor.heartbeatInterval', 10000) \\\n",
    "        .set('spark.network.timeout', 10000) \\\n",
    "        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Pyspark guide\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "filename_data = '1.csv'\n",
    "# Load the main data set into pyspark data frame \n",
    "df = spark.read.csv(filename_data, header=True, inferSchema=True, sep=';')\n",
    "print('Data frame type: ' + str(type(df)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:27:40.266969200Z",
     "start_time": "2024-01-18T15:27:36.368155Z"
    }
   },
   "id": "56f142476bfa567b",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|          timestamp|site_id|period_id|actual_consumption|         actual_pv|           load_00|           load_01|           load_02|           load_03|           load_04|           load_05|              pv_00|              pv_01|              pv_02|             pv_03|             pv_04|              pv_05|\n",
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|2014-07-19 18:45:00|      1|        0| 51.62570299494799| 22.71248932566911| 52.81682785868848| 53.50168799492416|54.079160878839055| 52.68347209706591| 52.59044527427321| 52.06742000601816|  18.32183627378414| 13.912748670447588| 10.946567612638287| 9.243136287606564| 6.962652902861995| 5.4669930866809695|\n",
      "|2014-07-19 19:30:00|      1|        0| 52.28125674965801| 6.618605013254157|51.452796259410526| 51.67628732173234|51.329881543538164| 51.69087874980712|51.538670816876014| 50.87881008186554|  6.339899092257709|  4.295641785016682|  3.016987171178838| 2.314616072296961| 2.015446493155706|  1.952004217554124|\n",
      "|2014-07-19 20:00:00|      1|        0| 50.71956514220455|1.4522094578011435| 51.31389786752856|52.199834619352444| 52.34054711591034| 51.84413830358043| 52.66106289252228| 52.53434003866664| 0.9361934717664752| 0.4031291503307324|  0.259490458744513|0.3389240759367292|0.4119713752310852|0.47937643931316587|\n",
      "|2014-07-19 20:15:00|      1|        0| 51.90116154382357|0.5808771932574699| 51.95047496345374| 51.62434500574838| 50.86743440161322| 51.53899692480025| 51.33116088544952|  50.8069631674964|0.21976110822772849|0.09104242159146397|0.18418205603352253|0.2698201627359923|0.3487915650400701| 0.4215291601455115|\n",
      "|2014-07-20 07:00:00|      1|        0| 53.52178000548263|1.1818485622166446| 53.12604298437412|52.905218588175764| 52.85315764111337| 52.71323810211246| 52.67487019009608|53.285592140681075| 3.5378355159881156| 6.3636723886913416|  13.95386221829731|22.670156313449596|  25.3259422256789|  34.43814171006084|\n",
      "|2014-07-20 08:00:00|      1|        0|  52.5358799466224|17.396101173102082|52.659622570819806| 53.36080348624089|52.767202030105736| 53.24035399655954| 52.80270765743882| 52.80739389975117| 22.304801671727706| 31.662820610265893| 35.012142452144396| 37.20567792121035|52.167527415971925|  52.92033590286843|\n",
      "|2014-07-20 08:30:00|      1|        0|53.116910326502335|30.367799714567802|  52.5413870687355| 53.02462681324922|52.592609534287774|52.600436836810616| 52.65692418620274| 52.61099602889929| 34.652520159534596| 36.875316814597376| 51.864046616796465|52.641548256185516|62.582957256236334|  73.31704208162886|\n",
      "|2014-07-20 09:15:00|      1|        0| 53.07272018473296|49.006964693800555| 52.98612914078394| 52.98993030405354| 52.91460283706227| 52.89990671125472| 52.88969785207387| 52.90268384886785|  52.61188194613109| 62.555704788243936|  73.29200704996114| 74.80331486969952| 76.42113776253701|  75.42574279282657|\n",
      "|2014-07-20 09:30:00|      1|        0| 53.36293544158401|55.534442155460866| 53.25501398737801| 53.11734441381144|53.067860898126455|53.038240399510215| 53.04039454755389| 53.01423488856077|  64.06395019079551|   74.6775318921919|  76.07610448682075| 77.59036501857337| 76.49983418850461|  97.21795795091992|\n",
      "|2014-07-20 10:45:00|      1|        0|53.240382781446215| 78.37247063325843|  53.0424817614937|52.945179481028475|53.322193240065566| 53.55982349783592|52.836880394620394|53.281319775741885|  96.54886601260827|  98.46710336697532|  97.34196073620033|  96.7818382238132|  96.2897767293757|  95.94863991705611|\n",
      "|2014-07-20 11:15:00|      1|        0| 53.23464406167797| 86.99013817099106| 53.42337302349482| 53.55593887121068|52.774368993964174| 53.18609417023272| 53.43534569672114| 52.79279640922453|  88.50715782341811|  88.66589177957817|  88.83419596136652| 89.09969322054447| 95.22499326939489|  95.67519566781796|\n",
      "|2014-07-20 11:30:00|      1|        0| 52.58019006162736| 83.35912143257191| 53.30971391287493|52.861251335815844|53.458853027319435| 53.81182512341558| 53.22715273632322|53.605899928597495|  86.00914507034867|  86.39361927608377|  86.85769757374337| 93.16542082875561| 93.78320330632828|   96.9059023337603|\n",
      "|2014-07-20 12:30:00|      1|        0| 52.92583295331312|   98.775390611724|52.516576877765715| 52.99320776303325| 52.68036166501977| 53.16299580013459|52.669652433087315| 52.68155829209582|    97.459531960225| 100.28310120972667|   97.6313954930457|  96.2361023713321| 97.71692888724819|  93.25175707448734|\n",
      "|2014-07-20 13:15:00|      1|        0|52.271558913115854|101.10595068504792|  52.8721037284065| 52.44455545599615|52.493175499604575| 52.52030586952272| 52.58273190718383| 52.54389249171494|  98.04346814581578|  99.37723574136382|  94.77697066917922| 99.22365508733691| 96.28313403792991|  95.02080906580655|\n",
      "|2014-07-20 13:45:00|      1|        0|52.310570210216355| 90.64550855161123| 52.43016128776264|52.496894057458746| 52.58141854826557| 52.55491024490528|52.566504964874575| 52.57297491057018|  91.32028002301931|  96.04822305614434|  93.36607560924885| 92.34110129919333|  90.5685220399317|  93.04234542896607|\n",
      "|2014-07-20 14:00:00|      1|        0|51.646678315634006| 102.4762868095902|52.111046368396934|52.417454500629134| 52.51475902884778| 52.59544229074434| 52.64046419074682|52.671131088721815| 101.80550254450722|  98.65490575671518|  97.19959816134623| 95.03170030457274| 97.14237076337312|  91.05378422892362|\n",
      "|2014-07-20 14:15:00|      1|        0| 52.98389999933044| 77.70079936250357| 52.68423941794403| 52.44335279749365| 52.33532271638462| 52.27504107438574|   52.246947760807|52.248383118657635|  86.21519360362838|   85.7720611112461|    84.533981271821| 87.49881376526812| 82.19488914080438|  85.93446299594257|\n",
      "|2014-07-20 15:00:00|      1|        0| 53.83115098894682| 80.31362251123201| 53.41996575659976| 53.19606545674153| 53.08823896338741| 53.50644709859108| 52.96010584933473| 52.95163505061517|  86.89951281155113|   81.6443511726841|  85.42872034283626| 76.82268308449626| 79.82676958396242|  75.05990570179004|\n",
      "|2014-07-20 15:45:00|      1|        0| 52.95170718111836| 43.83720184469408|53.371500930958916| 52.82604446724476|52.818067385874585| 52.84167138339948| 53.20957486433989| 53.46681295887524|  48.48516027023254|  53.79497014335376|  51.14622113035762| 49.89575685801035| 33.00117137976202| 32.211291884084865|\n",
      "|2014-07-20 17:00:00|      1|        0| 53.10239356499802|54.729442590914665|53.489024721350184|52.873908285813485|52.919678918055936| 53.56702989945684| 52.95469520532188|53.653789174512966| 50.289358251589455|  48.94105868039662| 47.548306747746224| 39.20637776952404|30.416255493510388| 24.911753027497593|\n",
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Define column names and data types\n",
    "columns = [\n",
    "    (\"timestamp\", TimestampType()),\n",
    "    (\"site_id\", IntegerType()),\n",
    "    (\"period_id\", IntegerType()),\n",
    "    (\"actual_consumption\", DoubleType()),\n",
    "    (\"actual_pv\", DoubleType()),\n",
    "    (\"load_00\", DoubleType()),\n",
    "    (\"load_01\", DoubleType()),\n",
    "    (\"load_02\", DoubleType()),\n",
    "    (\"load_03\", DoubleType()),\n",
    "    (\"load_04\", DoubleType()),\n",
    "    (\"load_05\", DoubleType()),\n",
    "    (\"pv_00\", DoubleType()),\n",
    "    (\"pv_01\", DoubleType()),\n",
    "    (\"pv_02\", DoubleType()),\n",
    "    (\"pv_03\", DoubleType()),\n",
    "    (\"pv_04\", DoubleType()),\n",
    "    (\"pv_05\", DoubleType())\n",
    "]\n",
    "\n",
    "# Apply the schema to the PySpark DataFrame\n",
    "for col, data_type in columns:\n",
    "    df = df.withColumn(col, df['`{}`'.format(col)].cast(data_type))\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns = [\"timestamp\", \"site_id\", \"period_id\", \"actual_consumption\", \"actual_pv\",\n",
    "                    \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                    \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "\n",
    "selected_df = df.select(*selected_columns)\n",
    "\n",
    "# Define conditions for filtering zeros and negative numbers\n",
    "conditions = (\n",
    "    (F.col(\"actual_consumption\") > 0) &\n",
    "    (F.col(\"actual_pv\") > 0) &\n",
    "    (F.col(\"load_00\") > 0) &\n",
    "    (F.col(\"load_01\") > 0) &\n",
    "    (F.col(\"load_02\") > 0) &\n",
    "    (F.col(\"load_03\") > 0) &\n",
    "    (F.col(\"load_04\") > 0) &\n",
    "    (F.col(\"load_05\") > 0) &\n",
    "    (F.col(\"pv_00\") > 0) &\n",
    "    (F.col(\"pv_01\") > 0) &\n",
    "    (F.col(\"pv_02\") > 0) &\n",
    "    (F.col(\"pv_03\") > 0) &\n",
    "    (F.col(\"pv_04\") > 0) &\n",
    "    (F.col(\"pv_05\") > 0)\n",
    ")\n",
    "\n",
    "# Apply the filter to the DataFrame\n",
    "filtered_df = selected_df.filter(conditions)\n",
    "\n",
    "# Specify the columns where you want to handle outliers (assuming these are numeric columns)\n",
    "numerical_columns = [\"actual_consumption\", \"actual_pv\", \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                     \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "\n",
    "# Define the lower and upper limits based on 3 standard deviations from the mean\n",
    "std_dev_limits = {col_name: 3 * filtered_df.agg({col_name: \"stddev\"}).collect()[0][0] for col_name in numerical_columns}\n",
    "mean_values = {col_name: filtered_df.agg({col_name: \"mean\"}).collect()[0][0] for col_name in numerical_columns}\n",
    "\n",
    "# Filter the data, keeping only the rows where values are within 3 standard deviations from the mean\n",
    "for col_name in numerical_columns:\n",
    "    lower_limit = mean_values[col_name] - std_dev_limits[col_name]\n",
    "    upper_limit = mean_values[col_name] + std_dev_limits[col_name]\n",
    "    filtered_df = filtered_df.filter((filtered_df[col_name] >= lower_limit) & (filtered_df[col_name] <= upper_limit))\n",
    "    \n",
    "    \n",
    "# Show the resulting DataFrame\n",
    "filtered_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:07.405412100Z",
     "start_time": "2024-01-18T15:27:40.274701600Z"
    }
   },
   "id": "6f2fe1f92619c2cd",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.5376\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Создайте вектор признаков из выбранных колонок\n",
    "feature_columns = [\"site_id\", \"period_id\", \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                   \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_new\")  # Изменено имя колонки\n",
    "assembled_df = vector_assembler.transform(filtered_df)\n",
    "\n",
    "# Разделите данные на обучающий и тестовый наборы\n",
    "(train_data, test_data) = assembled_df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Инициализируйте модель случайного леса\n",
    "rf = RandomForestRegressor(featuresCol=\"features_new\", labelCol=\"actual_consumption\")  \n",
    "\n",
    "# Создайте конвейер для последовательного выполнения шагов\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Обучите модель на обучающем наборе\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Сделайте прогнозы на тестовом наборе\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Оцените модель с использованием метрик регрессии\n",
    "evaluator = RegressionEvaluator(labelCol=\"actual_consumption\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:14.042966400Z",
     "start_time": "2024-01-18T15:28:07.398796300Z"
    }
   },
   "id": "7c683d3072d142e0",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    actual_consumption  prediction\n",
      "0            50.719565   53.450827\n",
      "1            53.116910   53.163483\n",
      "2            52.925833   53.163483\n",
      "3            52.271559   53.197487\n",
      "4            52.951707   53.197487\n",
      "5            52.280497   53.121900\n",
      "6            90.469777   89.486880\n",
      "7            90.177203   89.652281\n",
      "8            88.147575   88.264002\n",
      "9            89.307277   88.877156\n",
      "10           89.937456   89.652281\n",
      "11           71.385435   70.599884\n",
      "12           69.325334   60.634874\n",
      "13           90.764232   92.292888\n",
      "14           87.251615   89.031593\n",
      "15           93.231641   91.494730\n",
      "16           91.196495   93.283110\n",
      "17           92.045886   92.045176\n",
      "18           89.515550   84.690037\n",
      "19           91.700063   92.505897\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Преобразуйте результаты предсказаний в DataFrame\n",
    "predictions_df = predictions.select(\"actual_consumption\", \"prediction\").toPandas()\n",
    "\n",
    "# Выведите первые 20 строк DataFrame\n",
    "print(predictions_df.head(20))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:15.249165500Z",
     "start_time": "2024-01-18T15:28:14.039389600Z"
    }
   },
   "id": "4eda24e2287f1e79",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Создаем новую колонку \"label\" на основе условия\n",
    "filtered_df = filtered_df.withColumn(\"label\", when(filtered_df[\"actual_consumption\"] > 0, 1).otherwise(0))\n",
    "\n",
    "# Примените VectorAssembler и создайте новый DataFrame\n",
    "assembled_df = VectorAssembler(inputCols=feature_columns, outputCol=\"features\").transform(filtered_df).select(\"features\", \"label\")\n",
    "\n",
    "# Разделите данные на обучающий и тестовый наборы\n",
    "(train_data, test_data) = assembled_df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Инициализируйте модель Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Создайте конвейер для последовательного выполнения шагов\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Обучите модель на обучающем наборе\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Сделайте прогнозы на тестовом наборе\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Оцените модель с использованием метрик бинарной классификации\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % area_under_roc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:19.781983500Z",
     "start_time": "2024-01-18T15:28:15.247803300Z"
    }
   },
   "id": "2c72c31813b24049",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label  prediction probability\n",
      "0       1         1.0  [0.0, 1.0]\n",
      "1       1         1.0  [0.0, 1.0]\n",
      "2       1         1.0  [0.0, 1.0]\n",
      "3       1         1.0  [0.0, 1.0]\n",
      "4       1         1.0  [0.0, 1.0]\n",
      "5       1         1.0  [0.0, 1.0]\n",
      "6       1         1.0  [0.0, 1.0]\n",
      "7       1         1.0  [0.0, 1.0]\n",
      "8       1         1.0  [0.0, 1.0]\n",
      "9       1         1.0  [0.0, 1.0]\n",
      "10      1         1.0  [0.0, 1.0]\n",
      "11      1         1.0  [0.0, 1.0]\n",
      "12      1         1.0  [0.0, 1.0]\n",
      "13      1         1.0  [0.0, 1.0]\n",
      "14      1         1.0  [0.0, 1.0]\n",
      "15      1         1.0  [0.0, 1.0]\n",
      "16      1         1.0  [0.0, 1.0]\n",
      "17      1         1.0  [0.0, 1.0]\n",
      "18      1         1.0  [0.0, 1.0]\n",
      "19      1         1.0  [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Преобразуйте результаты предсказаний в Pandas DataFrame\n",
    "predictions_df = predictions.select(\"label\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "# Выведите первые несколько строк DataFrame\n",
    "print(predictions_df.head(20))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:20.990096600Z",
     "start_time": "2024-01-18T15:28:19.777354800Z"
    }
   },
   "id": "1c2c73a335c11ed2",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Останавливаем сессию Spark\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T15:28:21.178506800Z",
     "start_time": "2024-01-18T15:28:20.968124800Z"
    }
   },
   "id": "bb66a68c078fd891",
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
