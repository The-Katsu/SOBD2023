{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Выполните анализ Вами выбранного датасета с помощью двух алгоритмов машинного\n",
    "обучения в соответствии с индивидуальным вариантом:\n",
    "\n",
    "Вариант: 1  \n",
    "Задача регрессии: RandomForest  \n",
    "Задача бинарной классификации: LogisticRegression\n",
    "\n",
    "Датасет: 5. Датасет исторических данных по фотоэлектричеству и нагрузке."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "400fec1364b9ff29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:52:22.429057600Z",
     "start_time": "2024-01-20T13:52:22.407717300Z"
    }
   },
   "id": "6d3ae901be3a6eda",
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d832c93266f8615"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = '20G'\n",
    "# Initialize a spark session.\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n",
    "        .set('spark.executor.heartbeatInterval', 10000) \\\n",
    "        .set('spark.network.timeout', 10000) \\\n",
    "        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Pyspark guide\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "filename_data1 = '1.csv'\n",
    "filename_data2 = '2.csv'\n",
    "\n",
    "df1 = spark.read.csv(filename_data1, header=True, inferSchema=True, sep=';')\n",
    "df2 = spark.read.csv(filename_data2, header=True, inferSchema=True, sep=';')\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "print('Data frame type: ' + str(type(df)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:52:25.521705400Z",
     "start_time": "2024-01-20T13:52:22.425994800Z"
    }
   },
   "id": "56f142476bfa567b",
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|          timestamp|site_id|period_id|actual_consumption|         actual_pv|           load_00|           load_01|           load_02|           load_03|           load_04|           load_05|              pv_00|              pv_01|              pv_02|             pv_03|             pv_04|              pv_05|\n",
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n",
      "|2014-07-19 18:45:00|      1|        0| 51.62570299494799| 22.71248932566911| 52.81682785868848| 53.50168799492416|54.079160878839055| 52.68347209706591| 52.59044527427321| 52.06742000601816|  18.32183627378414| 13.912748670447588| 10.946567612638287| 9.243136287606564| 6.962652902861995| 5.4669930866809695|\n",
      "|2014-07-19 19:30:00|      1|        0| 52.28125674965801| 6.618605013254157|51.452796259410526| 51.67628732173234|51.329881543538164| 51.69087874980712|51.538670816876014| 50.87881008186554|  6.339899092257709|  4.295641785016682|  3.016987171178838| 2.314616072296961| 2.015446493155706|  1.952004217554124|\n",
      "|2014-07-19 20:00:00|      1|        0| 50.71956514220455|1.4522094578011435| 51.31389786752856|52.199834619352444| 52.34054711591034| 51.84413830358043| 52.66106289252228| 52.53434003866664| 0.9361934717664752| 0.4031291503307324|  0.259490458744513|0.3389240759367292|0.4119713752310852|0.47937643931316587|\n",
      "|2014-07-19 20:15:00|      1|        0| 51.90116154382357|0.5808771932574699| 51.95047496345374| 51.62434500574838| 50.86743440161322| 51.53899692480025| 51.33116088544952|  50.8069631674964|0.21976110822772849|0.09104242159146397|0.18418205603352253|0.2698201627359923|0.3487915650400701| 0.4215291601455115|\n",
      "|2014-07-20 07:00:00|      1|        0| 53.52178000548263|1.1818485622166446| 53.12604298437412|52.905218588175764| 52.85315764111337| 52.71323810211246| 52.67487019009608|53.285592140681075| 3.5378355159881156| 6.3636723886913416|  13.95386221829731|22.670156313449596|  25.3259422256789|  34.43814171006084|\n",
      "|2014-07-20 08:00:00|      1|        0|  52.5358799466224|17.396101173102082|52.659622570819806| 53.36080348624089|52.767202030105736| 53.24035399655954| 52.80270765743882| 52.80739389975117| 22.304801671727706| 31.662820610265893| 35.012142452144396| 37.20567792121035|52.167527415971925|  52.92033590286843|\n",
      "|2014-07-20 08:30:00|      1|        0|53.116910326502335|30.367799714567802|  52.5413870687355| 53.02462681324922|52.592609534287774|52.600436836810616| 52.65692418620274| 52.61099602889929| 34.652520159534596| 36.875316814597376| 51.864046616796465|52.641548256185516|62.582957256236334|  73.31704208162886|\n",
      "|2014-07-20 09:15:00|      1|        0| 53.07272018473296|49.006964693800555| 52.98612914078394| 52.98993030405354| 52.91460283706227| 52.89990671125472| 52.88969785207387| 52.90268384886785|  52.61188194613109| 62.555704788243936|  73.29200704996114| 74.80331486969952| 76.42113776253701|  75.42574279282657|\n",
      "|2014-07-20 09:30:00|      1|        0| 53.36293544158401|55.534442155460866| 53.25501398737801| 53.11734441381144|53.067860898126455|53.038240399510215| 53.04039454755389| 53.01423488856077|  64.06395019079551|   74.6775318921919|  76.07610448682075| 77.59036501857337| 76.49983418850461|  97.21795795091992|\n",
      "|2014-07-20 10:45:00|      1|        0|53.240382781446215| 78.37247063325843|  53.0424817614937|52.945179481028475|53.322193240065566| 53.55982349783592|52.836880394620394|53.281319775741885|  96.54886601260827|  98.46710336697532|  97.34196073620033|  96.7818382238132|  96.2897767293757|  95.94863991705611|\n",
      "|2014-07-20 11:15:00|      1|        0| 53.23464406167797| 86.99013817099106| 53.42337302349482| 53.55593887121068|52.774368993964174| 53.18609417023272| 53.43534569672114| 52.79279640922453|  88.50715782341811|  88.66589177957817|  88.83419596136652| 89.09969322054447| 95.22499326939489|  95.67519566781796|\n",
      "|2014-07-20 11:30:00|      1|        0| 52.58019006162736| 83.35912143257191| 53.30971391287493|52.861251335815844|53.458853027319435| 53.81182512341558| 53.22715273632322|53.605899928597495|  86.00914507034867|  86.39361927608377|  86.85769757374337| 93.16542082875561| 93.78320330632828|   96.9059023337603|\n",
      "|2014-07-20 12:30:00|      1|        0| 52.92583295331312|   98.775390611724|52.516576877765715| 52.99320776303325| 52.68036166501977| 53.16299580013459|52.669652433087315| 52.68155829209582|    97.459531960225| 100.28310120972667|   97.6313954930457|  96.2361023713321| 97.71692888724819|  93.25175707448734|\n",
      "|2014-07-20 13:15:00|      1|        0|52.271558913115854|101.10595068504792|  52.8721037284065| 52.44455545599615|52.493175499604575| 52.52030586952272| 52.58273190718383| 52.54389249171494|  98.04346814581578|  99.37723574136382|  94.77697066917922| 99.22365508733691| 96.28313403792991|  95.02080906580655|\n",
      "|2014-07-20 13:45:00|      1|        0|52.310570210216355| 90.64550855161123| 52.43016128776264|52.496894057458746| 52.58141854826557| 52.55491024490528|52.566504964874575| 52.57297491057018|  91.32028002301931|  96.04822305614434|  93.36607560924885| 92.34110129919333|  90.5685220399317|  93.04234542896607|\n",
      "|2014-07-20 14:00:00|      1|        0|51.646678315634006| 102.4762868095902|52.111046368396934|52.417454500629134| 52.51475902884778| 52.59544229074434| 52.64046419074682|52.671131088721815| 101.80550254450722|  98.65490575671518|  97.19959816134623| 95.03170030457274| 97.14237076337312|  91.05378422892362|\n",
      "|2014-07-20 14:15:00|      1|        0| 52.98389999933044| 77.70079936250357| 52.68423941794403| 52.44335279749365| 52.33532271638462| 52.27504107438574|   52.246947760807|52.248383118657635|  86.21519360362838|   85.7720611112461|    84.533981271821| 87.49881376526812| 82.19488914080438|  85.93446299594257|\n",
      "|2014-07-20 15:00:00|      1|        0| 53.83115098894682| 80.31362251123201| 53.41996575659976| 53.19606545674153| 53.08823896338741| 53.50644709859108| 52.96010584933473| 52.95163505061517|  86.89951281155113|   81.6443511726841|  85.42872034283626| 76.82268308449626| 79.82676958396242|  75.05990570179004|\n",
      "|2014-07-20 15:45:00|      1|        0| 52.95170718111836| 43.83720184469408|53.371500930958916| 52.82604446724476|52.818067385874585| 52.84167138339948| 53.20957486433989| 53.46681295887524|  48.48516027023254|  53.79497014335376|  51.14622113035762| 49.89575685801035| 33.00117137976202| 32.211291884084865|\n",
      "|2014-07-20 17:00:00|      1|        0| 53.10239356499802|54.729442590914665|53.489024721350184|52.873908285813485|52.919678918055936| 53.56702989945684| 52.95469520532188|53.653789174512966| 50.289358251589455|  48.94105868039662| 47.548306747746224| 39.20637776952404|30.416255493510388| 24.911753027497593|\n",
      "+-------------------+-------+---------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Define column names and data types\n",
    "columns = [\n",
    "    (\"timestamp\", TimestampType()),\n",
    "    (\"site_id\", IntegerType()),\n",
    "    (\"period_id\", IntegerType()),\n",
    "    (\"actual_consumption\", DoubleType()),\n",
    "    (\"actual_pv\", DoubleType()),\n",
    "    (\"load_00\", DoubleType()),\n",
    "    (\"load_01\", DoubleType()),\n",
    "    (\"load_02\", DoubleType()),\n",
    "    (\"load_03\", DoubleType()),\n",
    "    (\"load_04\", DoubleType()),\n",
    "    (\"load_05\", DoubleType()),\n",
    "    (\"pv_00\", DoubleType()),\n",
    "    (\"pv_01\", DoubleType()),\n",
    "    (\"pv_02\", DoubleType()),\n",
    "    (\"pv_03\", DoubleType()),\n",
    "    (\"pv_04\", DoubleType()),\n",
    "    (\"pv_05\", DoubleType())\n",
    "]\n",
    "\n",
    "# Apply the schema to the PySpark DataFrame\n",
    "for col, data_type in columns:\n",
    "    df = df.withColumn(col, df['`{}`'.format(col)].cast(data_type))\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns = [\"timestamp\", \"site_id\", \"period_id\", \"actual_consumption\", \"actual_pv\",\n",
    "                    \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                    \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "\n",
    "selected_df = df.select(*selected_columns)\n",
    "\n",
    "# Define conditions for filtering zeros and negative numbers\n",
    "conditions = (\n",
    "    (F.col(\"actual_consumption\") > 0) &\n",
    "    (F.col(\"actual_pv\") > 0) &\n",
    "    (F.col(\"load_00\") > 0) &\n",
    "    (F.col(\"load_01\") > 0) &\n",
    "    (F.col(\"load_02\") > 0) &\n",
    "    (F.col(\"load_03\") > 0) &\n",
    "    (F.col(\"load_04\") > 0) &\n",
    "    (F.col(\"load_05\") > 0) &\n",
    "    (F.col(\"pv_00\") > 0) &\n",
    "    (F.col(\"pv_01\") > 0) &\n",
    "    (F.col(\"pv_02\") > 0) &\n",
    "    (F.col(\"pv_03\") > 0) &\n",
    "    (F.col(\"pv_04\") > 0) &\n",
    "    (F.col(\"pv_05\") > 0)\n",
    ")\n",
    "\n",
    "# Apply the filter to the DataFrame\n",
    "filtered_df = selected_df.filter(conditions)\n",
    "\n",
    "# Specify the columns where you want to handle outliers (assuming these are numeric columns)\n",
    "numerical_columns = [\"actual_consumption\", \"actual_pv\", \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                     \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "\n",
    "# Define the lower and upper limits based on 3 standard deviations from the mean\n",
    "std_dev_limits = {col_name: 3 * filtered_df.agg({col_name: \"stddev\"}).collect()[0][0] for col_name in numerical_columns}\n",
    "mean_values = {col_name: filtered_df.agg({col_name: \"mean\"}).collect()[0][0] for col_name in numerical_columns}\n",
    "\n",
    "# Filter the data, keeping only the rows where values are within 3 standard deviations from the mean\n",
    "for col_name in numerical_columns:\n",
    "    lower_limit = mean_values[col_name] - std_dev_limits[col_name]\n",
    "    upper_limit = mean_values[col_name] + std_dev_limits[col_name]\n",
    "    filtered_df = filtered_df.filter((filtered_df[col_name] >= lower_limit) & (filtered_df[col_name] <= upper_limit))\n",
    "    \n",
    "    \n",
    "# Show the resulting DataFrame\n",
    "filtered_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:52:50.963579500Z",
     "start_time": "2024-01-20T13:52:25.524112900Z"
    }
   },
   "id": "6f2fe1f92619c2cd",
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (CV) - Root Mean Squared Error (RMSE) on test data = 2.20308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Определите категориальные признаки\n",
    "categorical_features = [\"site_id\", \"period_id\"]\n",
    "\n",
    "# Инициализируйте StringIndexer для индексации категориальных признаков\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=f\"{column}_index\", handleInvalid=\"skip\") for column in categorical_features]\n",
    "\n",
    "# Инициализируйте OneHotEncoder для кодирования категориальных признаков\n",
    "encoder_rf = OneHotEncoder(inputCols=[f\"{col}_index\" for col in categorical_features],\n",
    "                           outputCols=[f\"{col}_encoded\" for col in categorical_features])\n",
    "\n",
    "# Примените VectorAssembler для объединения всех признаков, включая закодированные\n",
    "feature_columns_rf = [\"site_id_encoded\", \"period_id_encoded\", \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                      \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "vector_assembler_rf = VectorAssembler(inputCols=feature_columns_rf, outputCol=\"features_new\")\n",
    "\n",
    "# Разделите данные на обучающий и тестовый наборы для RandomForestRegressor\n",
    "(train_data_rf, test_data_rf) = filtered_df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Инициализируйте модель случайного леса\n",
    "rf = RandomForestRegressor(featuresCol=\"features_new\", labelCol=\"actual_consumption\")\n",
    "\n",
    "# Создайте конвейер для последовательного выполнения шагов\n",
    "pipeline_rf = Pipeline(stages=indexers + [encoder_rf, vector_assembler_rf, rf])\n",
    "\n",
    "# Параметры для подбора\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Оценщик для оценки качества модели (RMSE)\n",
    "evaluator_rf = RegressionEvaluator(labelCol=\"actual_consumption\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Кросс-валидация для RandomForestRegressor\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                             estimatorParamMaps=param_grid_rf,\n",
    "                             evaluator=evaluator_rf,\n",
    "                             numFolds=3)\n",
    "\n",
    "# Обучение модели\n",
    "model_cv_rf = crossval_rf.fit(train_data_rf)\n",
    "predictions_cv_rf = model_cv_rf.transform(test_data_rf)\n",
    "\n",
    "# Оценка модели с использованием метрик регрессии\n",
    "rmse_cv_rf = evaluator_rf.evaluate(predictions_cv_rf)\n",
    "print(\"Random Forest (CV) - Root Mean Squared Error (RMSE) on test data = %g\" % rmse_cv_rf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:54:29.528434300Z",
     "start_time": "2024-01-20T13:52:50.977458900Z"
    }
   },
   "id": "7c683d3072d142e0",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    actual_consumption  prediction\n",
      "0            50.719565   53.313102\n",
      "1            53.116910   53.030801\n",
      "2            52.925833   53.056896\n",
      "3            52.271559   53.279963\n",
      "4            52.951707   53.080376\n",
      "5            52.280497   52.554910\n",
      "6            90.469777   89.799751\n",
      "7            90.177203   90.878125\n",
      "8            88.147575   88.605385\n",
      "9            89.307277   88.605385\n",
      "10           89.937456   89.786369\n",
      "11           71.385435   73.930008\n",
      "12           69.325334   63.047479\n",
      "13           90.764232   91.795022\n",
      "14           87.251615   89.419231\n",
      "15           93.231641   91.444040\n",
      "16           91.196495   93.051722\n",
      "17           92.045886   91.188915\n",
      "18           89.515550   85.497058\n",
      "19           91.700063   92.797852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Преобразуйте результаты предсказаний в DataFrame\n",
    "predictions_df = predictions_cv_rf.select(\"actual_consumption\", \"prediction\").limit(20).toPandas()\n",
    "\n",
    "# Выведите первые 20 строк DataFrame\n",
    "print(predictions_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:54:30.215667200Z",
     "start_time": "2024-01-20T13:54:29.536429900Z"
    }
   },
   "id": "4eda24e2287f1e79",
   "execution_count": 152
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.999475\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, mean, when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Рассчитаем среднее значение actual_consumption\n",
    "mean_actual_consumption = filtered_df.select(mean(\"actual_consumption\")).collect()[0][0]\n",
    "\n",
    "# Создадим новый столбец \"label\" на основе условия (> среднего значения)\n",
    "filtered_df = filtered_df.withColumn(\"label\", when(col(\"actual_consumption\") > mean_actual_consumption, 1).otherwise(0))\n",
    "\n",
    "# Определим категориальные признаки\n",
    "categorical_features = [\"site_id\", \"period_id\"]\n",
    "\n",
    "# Инициализируем StringIndexer для индексации категориальных признаков\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=f\"{column}_index\", handleInvalid=\"skip\") for column in categorical_features]\n",
    "\n",
    "# Инициализируем OneHotEncoder для кодирования категориальных признаков\n",
    "encoder = OneHotEncoder(inputCols=[f\"{col}_index\" for col in categorical_features],\n",
    "                        outputCols=[f\"{col}_encoded\" for col in categorical_features])\n",
    "\n",
    "# Применяем VectorAssembler для объединения всех признаков, включая закодированные\n",
    "feature_columns = [\"site_id_encoded\", \"period_id_encoded\", \"load_00\", \"load_01\", \"load_02\", \"load_03\", \"load_04\", \"load_05\",\n",
    "                   \"pv_00\", \"pv_01\", \"pv_02\", \"pv_03\", \"pv_04\", \"pv_05\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Применяем OneHotEncoder и VectorAssembler\n",
    "assembled_df = Pipeline(stages=indexers + [encoder, vector_assembler]).fit(filtered_df).transform(filtered_df).select(\"features\", \"label\")\n",
    "\n",
    "# Разделяем данные на обучающий и тестовый наборы\n",
    "(train_data, test_data) = assembled_df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Инициализируем модель Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Создаем конвейер для последовательного выполнения шагов\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Задаем сетку параметров для поиска\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Инициализируем оценщик бинарной классификации\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Инициализируем кросс-валидатор с 5 фолдами\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=5)\n",
    "\n",
    "# Обучаем модель на обучающем наборе с использованием кросс-валидации и подбора параметров\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "\n",
    "# Проводим оценку модели на тестовом наборе\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Вычисляем значение площади под ROC-кривой\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % area_under_roc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:57:01.665224700Z",
     "start_time": "2024-01-20T13:54:30.216973900Z"
    }
   },
   "id": "2c72c31813b24049",
   "execution_count": 153
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label  prediction                                probability\n",
      "0       0         0.0  [0.5304489898308216, 0.46955101016917844]\n",
      "1       0         0.0  [0.5312680479654897, 0.46873195203451035]\n",
      "2       0         0.0  [0.5352670278134856, 0.46473297218651444]\n",
      "3       0         0.0   [0.5357014566416268, 0.4642985433583732]\n",
      "4       0         0.0   [0.537338935244245, 0.46266106475575497]\n",
      "5       0         0.0   [0.5351485605838534, 0.4648514394161466]\n",
      "6       0         0.0   [0.5354243449122519, 0.4645756550877481]\n",
      "7       0         0.0     [0.534155499134751, 0.465844500865249]\n",
      "8       0         0.0   [0.531868299944354, 0.46813170005564597]\n",
      "9       0         0.0  [0.5333508030575281, 0.46664919694247187]\n",
      "10      0         0.0     [0.530624068336641, 0.469375931663359]\n",
      "11      0         0.0   [0.5301406586422228, 0.4698593413577772]\n",
      "12      0         0.0  [0.5346317098207232, 0.46536829017927683]\n",
      "13      0         0.0   [0.5304265188566696, 0.4695734811433304]\n",
      "14      0         0.0  [0.5313675026903036, 0.46863249730969636]\n",
      "15      0         0.0  [0.5324660238463534, 0.46753397615364656]\n",
      "16      0         0.0     [0.527634377283841, 0.472365622716159]\n",
      "17      0         0.0   [0.5245754470861005, 0.4754245529138995]\n",
      "18      0         0.0  [0.5275746282548948, 0.47242537174510524]\n",
      "19      0         0.0  [0.5210938772603609, 0.47890612273963906]\n"
     ]
    }
   ],
   "source": [
    "# Преобразуйте результаты предсказаний в Pandas DataFrame\n",
    "predictions_df = predictions.select(\"label\", \"prediction\", \"probability\").limit(20).toPandas()\n",
    "\n",
    "# Выведите первые несколько строк DataFrame\n",
    "print(predictions_df.head(20))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:57:02.426927200Z",
     "start_time": "2024-01-20T13:57:01.664588700Z"
    }
   },
   "id": "1c2c73a335c11ed2",
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9769\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Оценка модели с использованием метрик\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score: {:.4f}\".format(f1_score))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:57:03.779908300Z",
     "start_time": "2024-01-20T13:57:02.434461Z"
    }
   },
   "id": "e91a37e29904f3f0",
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1 Score равный 0.9554 является довольно высоким показателем и может свидетельствовать о хорошей производительности модели на задаче классификации. F1 Score - это метрика, которая учитывает как точность (precision), так и полноту (recall) модели. Значение 0.9554 говорит о том, что модель успешно сбалансировала компромисс между точностью и полнотой при классификации."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "686405fd1760818b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9769388134647573\n",
      "Precision for class 1: 0.9587140439932318\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Преобразуем DataFrame с предсказаниями в RDD\n",
    "prediction_and_label = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "\n",
    "# Инициализируем объект MulticlassMetrics\n",
    "metrics = MulticlassMetrics(prediction_and_label)\n",
    "\n",
    "# Оценка Accuracy\n",
    "accuracy = metrics.accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Оценка Precision для класса 1\n",
    "precision = metrics.precision(1.0)\n",
    "print(f\"Precision for class 1: {precision}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:57:08.184224300Z",
     "start_time": "2024-01-20T13:57:03.790510100Z"
    }
   },
   "id": "ae077c80e454e24",
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy: 0.956 (95.6%) - это общая точность модели, то есть доля правильных предсказаний по всем классам.\n",
    "Precision for class 1: 0.935 (93.5%) - это precision (точность) для положительного класса. Это показывает, какую долю из предсказанных как положительные объектов на самом деле являются положительными.\n",
    "Общий уровень точности высок, и точность в предсказании положительного класса тоже хороша, но, конечно, оценка зависит от контекста задачи."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1e37f7a47c515be"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b4cb133b7c2368"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Останавливаем сессию Spark\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T13:57:09.176826900Z",
     "start_time": "2024-01-20T13:57:08.189126400Z"
    }
   },
   "id": "bb66a68c078fd891",
   "execution_count": 157
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
